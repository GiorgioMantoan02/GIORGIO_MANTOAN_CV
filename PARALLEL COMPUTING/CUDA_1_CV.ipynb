{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CUDA 1**"
      ],
      "metadata": {
        "id": "xvdtBiPrGllf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Implementation* of a **batched arbitrarily-size matrix multiplication** kernel.\n",
        "\n",
        "Let dimensions be identical for all matrix multiplications in the batch.<br>\n",
        "These being $m, k, n \\in \\mathbb{N}$.<br>\n",
        "While $batch \\in \\mathbb{N}$ is the batch size.\n",
        "\n",
        "You are provided as input the following matrices:<br>\n",
        "$N_0, N_1, N_2, ... N_{batch - 1} \\in \\mathbb{M}^{k \\times n}$<br>\n",
        "$M \\in \\mathbb{M}^{m \\times k}$\n",
        "\n",
        "You need to compute:<br>\n",
        "$P_0, P_1, P_2, ... P_{batch - 1} \\in \\mathbb{M}^{m \\times n}$\n",
        "\n",
        "Where $P_i = M \\otimes N_i$ for each $i \\in \\{0, ..., batch - 1\\}$.\n",
        "\n",
        "---\n",
        "\n",
        "To get a general performance metric rely on the profiler, specifically look for the `cuda_gpu_kern_sum` and try to minimize the `Total Time (ns)` of your kernel. You may also want to improve `cuda_gpu_mem_time_sum`.\n",
        "\n",
        "Step one is beating the reference implementation, that should be easy, then you can use all tricks in the book to push it further.\n",
        "Anything goes, but if you use \"exotic\" tricks we want an explanation.\n",
        "In fact, submitting your work, be sure to fill out the [report](#report) with brief insights of what you did."
      ],
      "metadata": {
        "id": "xcDOQnQMGwLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Colab Setup**"
      ],
      "metadata": {
        "id": "iHGYx97cJqaE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt update\n",
        "!apt install ./nsight-systems-2023.2.3_2023.2.3.1001-1_amd64.deb\n",
        "!apt --fix-broken install"
      ],
      "metadata": {
        "id": "MWrw0NgzGlMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITAYKD7MGcmH"
      },
      "outputs": [],
      "source": [
        "!mkdir /home/cuda\n",
        "%cd /home/cuda"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Code**"
      ],
      "metadata": {
        "id": "F9rbKG58Jyw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bmatmul.cpp\n",
        "// DON'T CHANGE THIS ^^ FILENAME!\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <math.h>\n",
        "\n",
        "// utility for wrapping CUDA API calls and log any error they may return (use this for debugging)\n",
        "#define gpuErrchk(ans) { gpuAssert((ans), __FILE__, __LINE__); }\n",
        "inline void gpuAssert(cudaError_t code, const char *file, int line, bool abort = true) {\n",
        "  if (code != cudaSuccess) {\n",
        "    fprintf(stderr,\"GPUassert: %s %s %d\\n\", cudaGetErrorString(code), file, line);\n",
        "    if (abort)\n",
        "      exit(code);\n",
        "  }\n",
        "}\n",
        "\n",
        "// === DO NOT CHANGE THIS ===\n",
        "// host-side version, used to validate results\n",
        "__host__\n",
        "void batchedMatMulHost(float* M, float* N, float* P, int m, int k, int n, int batch) {\n",
        "  for (int b = 0; b < batch; b++) {\n",
        "    for (int row = 0; row < m; row++) {\n",
        "      for (int col = 0; col < n; col++) {\n",
        "        float value = 0.0f;\n",
        "        for (int i = 0; i < k; i++) {\n",
        "          float a = M[row*k + i];\n",
        "          float c = N[b*(k*n) + i*n + col];\n",
        "          value += a * c;\n",
        "        }\n",
        "        P[b*(m*n) + row*n + col] = value;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initWith(float number, float* arr, int size) {\n",
        "  for (int i = 0; i < size; i++)\n",
        "    arr[i] = number;\n",
        "}\n",
        "\n",
        "void initRandom(float* arr, int size, unsigned int seed, float minVal = 0.0f, float maxVal = 1.0f) {\n",
        "  srand(seed);\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float r = (float)rand() / RAND_MAX;\n",
        "    arr[i] = minVal + r * (maxVal - minVal);\n",
        "  }\n",
        "}\n",
        "\n",
        "void checkResult(float* arr1, float* arr2, int size) {\n",
        "  const float atol = 1e-4f; // absolute tolerance for fp32 (lack of) associativity\n",
        "  const float rtol = 1e-4f; // relative tolerance for fp32 (lack of) associativity\n",
        "  for (int i = 0; i < size; i++) {\n",
        "    float diff = fabs(arr1[i] - arr2[i]);\n",
        "    float tol = atol + rtol*fabs(arr2[i]);\n",
        "    if (diff > tol) {\n",
        "      printf(\"Error at %d: %f != %f (diff=%e, tol=%e)\\n\", i, arr1[i], arr2[i], diff, tol);\n",
        "      exit(1);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "// ==========================\n",
        "\n",
        "#define TILE_WIDTH 16\n",
        "#define BLOCK_WIDTH 16\n",
        "#define BLOCK_HEIGHT 16\n",
        "\n",
        "__global__ void batchedMatMul(\n",
        "        const float * __restrict__ M,\n",
        "        const float * __restrict__ N,\n",
        "        float * const __restrict__ P,\n",
        "        int m, int k, int n, int batch) {\n",
        "    __shared__ float Mds[TILE_WIDTH*2][TILE_WIDTH];\n",
        "    __shared__ float Nds[TILE_WIDTH][TILE_WIDTH*2];\n",
        "\n",
        "    int bx = blockIdx.x, by = blockIdx.y, b = blockIdx.z;\n",
        "    int tx = threadIdx.x, ty = threadIdx.y;\n",
        "\n",
        "    int Row = by * BLOCK_HEIGHT * 2 + ty * 2;\n",
        "    int Col = bx * BLOCK_WIDTH * 2 + tx * 2;\n",
        "\n",
        "    float P00 = 0.0f, P01 = 0.0f, P10 = 0.0f, P11 = 0.0f;\n",
        "\n",
        "    for (int t = 0; t < (k + TILE_WIDTH - 1) / TILE_WIDTH; ++t) {\n",
        "        int tiledCol = t * TILE_WIDTH + tx;\n",
        "        int tiledRow = t * TILE_WIDTH + ty;\n",
        "\n",
        "        // Ogni thread carica due righe di M e due colonne di N\n",
        "        Mds[ty*2][tx] = ((Row < m && tiledCol < k) ? M[Row * k + tiledCol] : 0.0f);\n",
        "        Mds[ty*2 + 1][tx] = ((Row + 1 < m && tiledCol < k) ? M[(Row + 1) * k + tiledCol] : 0.0f);\n",
        "\n",
        "        Nds[ty][tx*2] = ((Col < n && tiledRow < k) ? N[b * k * n + tiledRow * n + Col] : 0.0f);\n",
        "        Nds[ty][tx*2 + 1] = ((Col + 1 < n && tiledRow < k) ? N[b * k * n + tiledRow * n + Col + 1] : 0.0f);\n",
        "\n",
        "        __syncthreads();\n",
        "\n",
        "        for (int i = 0; i < TILE_WIDTH; ++i) {\n",
        "            float a0 = Mds[ty*2][i];\n",
        "            float a1 = Mds[ty*2 + 1][i];\n",
        "            float b0 = Nds[i][tx*2];\n",
        "            float b1 = Nds[i][tx*2 + 1];\n",
        "\n",
        "            P00 += a0 * b0;\n",
        "            P01 += a0 * b1;\n",
        "            P10 += a1 * b0;\n",
        "            P11 += a1 * b1;\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (Row < m && Col < n) {\n",
        "        P[b * m * n + Row * n + Col] = P00;\n",
        "    }\n",
        "    if (Row < m && Col + 1 < n) {\n",
        "        P[b * m * n + Row * n + Col + 1] = P01;\n",
        "    }\n",
        "    if (Row + 1 < m && Col < n) {\n",
        "        P[b * m * n + (Row + 1) * n + Col] = P10;\n",
        "    }\n",
        "    if (Row + 1 < m && Col + 1 < n) {\n",
        "        P[b * m * n + (Row + 1) * n + Col + 1] = P11;\n",
        "    }\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "int main(int argc, char** argv) {\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  if (argc != 6) {\n",
        "    printf(\"Usage: %s <m> <k> <n> <batch> <seed>\\n\", argv[0]);\n",
        "    exit(1);\n",
        "  }\n",
        "\n",
        "  int m = atoi(argv[1]); // rows of Ms and Ps\n",
        "  int k = atoi(argv[2]); // cols of Ms, rows of Ns\n",
        "  int n = atoi(argv[3]); // cols of Ns and Ps\n",
        "  int batch = atoi(argv[4]); // number of matrix pairs\n",
        "  unsigned int seed = (unsigned int)atoi(argv[5]); // seed for random initialization\n",
        "\n",
        "  printf(\"Running batched matmul with m=%d, k=%d, n=%d, batch=%d, seed=%u\\n\", m, k, n, batch, seed);\n",
        "\n",
        "  const int sizeM = m*k;\n",
        "  const int sizeN = k*n*batch;\n",
        "  const int sizeP = m*n*batch;\n",
        "  // ==========================\n",
        "  // initialize\n",
        "  float *M, *N, *P;\n",
        "  gpuErrchk(cudaMallocHost((void**)&M, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMallocHost((void**)&N, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMallocHost((void**)&P, sizeP * sizeof(float)));\n",
        "\n",
        "  initRandom(M, sizeM, seed);\n",
        "  initRandom(N, sizeN, seed + 1);\n",
        "  initWith(0.0f, P, sizeP);\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything\n",
        "  float *M_d;\n",
        "  float *N_d;\n",
        "  float *P_d;\n",
        "\n",
        "  gpuErrchk(cudaMalloc((void**)&M_d, sizeM * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&N_d, sizeN * sizeof(float)));\n",
        "  gpuErrchk(cudaMalloc((void**)&P_d, sizeP * sizeof(float)));\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(M_d, M, sizeM * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(N_d, N, sizeN * sizeof(float), cudaMemcpyHostToDevice));\n",
        "  gpuErrchk(cudaMemcpy(P_d, P, sizeP * sizeof(float), cudaMemcpyHostToDevice));\n",
        "\n",
        "  //dim3 numBlocks((n + blockSize.x - 1) / blockSize.x, (m + blockSize.y - 1) / blockSize.y);\n",
        "  dim3 blockSize(BLOCK_WIDTH, BLOCK_HEIGHT);\n",
        "  dim3 gridSize((n + BLOCK_WIDTH * 2 - 1) / (BLOCK_WIDTH * 2), (m + BLOCK_HEIGHT * 2 - 1) / (BLOCK_HEIGHT * 2), batch);\n",
        "\n",
        "  batchedMatMul<<<gridSize, blockSize>>>(M_d, N_d, P_d, m, k, n, batch);\n",
        "  gpuErrchk(cudaDeviceSynchronize());\n",
        "\n",
        "  gpuErrchk(cudaMemcpy(P, P_d, sizeP * sizeof(float), cudaMemcpyDeviceToHost));\n",
        "\n",
        "  // === DO NOT CHANGE THIS ===\n",
        "  // However: once you know results are correct, you can temporarily\n",
        "  //          comment this out if you want to test performance on large\n",
        "  //          matrices, since the evaluation on CPU can get pretty slow.\n",
        "  printf(\"Checking results on CPU...\\n\");\n",
        "  float* P_host = (float*)malloc(sizeP * sizeof(float));\n",
        "  initWith(0.0f, P_host, sizeP);\n",
        "  batchedMatMulHost(M, N, P_host, m, k, n, batch);\n",
        "  checkResult(P, P_host, m*n*batch);\n",
        "  printf(\"All results matched, success!\");\n",
        "  // ==========================\n",
        "\n",
        "  // here, you can change anything, e.g. add some logging\n",
        "  gpuErrchk(cudaFree(M_d));\n",
        "  gpuErrchk(cudaFree(N_d));\n",
        "  gpuErrchk(cudaFree(P_d));\n",
        "\n",
        "  cudaFreeHost(M);\n",
        "  cudaFreeHost(N);\n",
        "  cudaFreeHost(P);\n",
        "  free(P_host);\n",
        "\n",
        "  return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "6Ys4rptyJ5EJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Compile, Run, Profile**"
      ],
      "metadata": {
        "id": "t4eaFcePJ_8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compile and run:"
      ],
      "metadata": {
        "id": "gXV_BmFYKM2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!mv bmatmul.cpp bmatmul.cu\n",
        "!nvcc -arch=sm_75 bmatmul.cu -o bmatmul"
      ],
      "metadata": {
        "id": "PKs3tgz6KDYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./bmatmul 123 456 678 3 1"
      ],
      "metadata": {
        "id": "TlgkX-SPZhqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Profile:"
      ],
      "metadata": {
        "id": "SBn4aD0gKRq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nsys profile --stats=true ./bmatmul 256 512 384 16 119"
      ],
      "metadata": {
        "id": "VRLC6R5AKRTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"report\"></a>\n",
        "## **Brief Report**"
      ],
      "metadata": {
        "id": "edxNOq-8PCdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**TEAM information:**\n",
        "- member-1: GIORGIO, MANTOAN\n",
        "- member-2: ANDREA, OGGIONI\n",
        "- member-3: MATTEO, PARIMBELLI\n"
      ],
      "metadata": {
        "id": "5COx5mKMPF7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Used shared memory to reduce global memory access (shared is faster than global).\n",
        "- Use pinned memory to speedup memory transfers.\n",
        "- Multiplication is performed by tiling.\n",
        "- Batching implemented using the `z` coordinate of the grid.\n",
        "- Use of micro tiling to improve speed (more math per memory access).\n",
        "- Use of `const` and `__restrict__` pointers where possible to encourage the compiler to perform further minor optimizations.\n",
        "\n",
        "- Tried `#pragma unroll` on the inner for-loop but kernel was 2$\\mu$s slower.\n"
      ],
      "metadata": {
        "id": "PRKJGx2MR7LQ"
      }
    }
  ]
}